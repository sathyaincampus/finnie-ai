# Finnie AI â€” Code Walkthrough

> **Last updated:** 2026-02-08  
> A comprehensive guide to the Finnie AI codebase for developers and reviewers.

---

## Architecture Overview

```mermaid
graph TD
    subgraph UI["ğŸ–¥ï¸ UI Layer"]
        ST["Streamlit App<br/>src/ui/app.py"]
        AUTH["OAuth Auth<br/>src/ui/auth.py"]
        VOICE["Voice I/O<br/>src/ui/voice.py"]
        STT["STT Component<br/>src/ui/stt_component/"]
    end

    subgraph ORCH["ğŸ”€ Orchestration"]
        ROUTER["Intent Router<br/>generate_response()"]
        GRAPH["LangGraph Workflow<br/>src/orchestration/graph.py"]
        STATE["State Management<br/>src/orchestration/state.py"]
        NODES["Node Functions<br/>src/orchestration/nodes.py"]
    end

    subgraph AGENTS["ğŸ¤– Agent Team"]
        QUANT["ğŸ“Š Quant<br/>Market Data"]
        PROF["ğŸ“š Professor<br/>Education"]
        SCOUT["ğŸŒ Scout<br/>Trends"]
        ORACLE["ğŸ”® Oracle<br/>Projections"]
        ADVISOR["ğŸ’¼ Advisor<br/>Portfolio"]
        ANALYST["ğŸ“° Analyst<br/>Research"]
        GUARD["ğŸ›¡ï¸ Guardian<br/>Compliance"]
        SCRIBE["âœï¸ Scribe<br/>Synthesis"]
    end

    subgraph LLM["ğŸ§  LLM Layer"]
        ADAPTER["Adapter Factory<br/>src/llm/adapter.py"]
        OAI["OpenAI"]
        ANT["Anthropic"]
        GOOG["Google Gemini"]
    end

    subgraph TOOLS["ğŸ”§ Tool Layer"]
        MCP["MCP Server<br/>src/mcp/server.py"]
        FIN["Finance Tools<br/>yFinance"]
        CHART["Chart Tools<br/>Plotly"]
    end

    subgraph DATA["ğŸ’¾ Data Layer"]
        MEM["Memory<br/>SQLite"]
        OBS["Observability<br/>LangFuse"]
        CFG["Config<br/>.env + pydantic"]
    end

    ST --> AUTH
    ST --> VOICE
    VOICE --> STT
    ST --> ROUTER
    ROUTER --> AGENTS
    AGENTS --> ADAPTER
    ADAPTER --> OAI & ANT & GOOG
    AGENTS --> MCP
    MCP --> FIN & CHART
    ST --> MEM
    ROUTER --> OBS
    CFG --> ST
```

---

## Project Structure

```
finnie-ai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py              # Centralized config via pydantic-settings
â”‚   â”œâ”€â”€ memory.py              # SQLite chat persistence layer
â”‚   â”œâ”€â”€ observability.py       # LangFuse tracing & metrics
â”‚   â”œâ”€â”€ agents/                # Multi-agent team
â”‚   â”‚   â”œâ”€â”€ base.py            # BaseFinnieAgent ABC
â”‚   â”‚   â”œâ”€â”€ quant.py           # ğŸ“Š Market data (yFinance)
â”‚   â”‚   â”œâ”€â”€ professor.py       # ğŸ“š Financial education
â”‚   â”‚   â”œâ”€â”€ scout.py           # ğŸŒ Trend discovery
â”‚   â”‚   â”œâ”€â”€ oracle.py          # ğŸ”® Monte Carlo projections
â”‚   â”‚   â”œâ”€â”€ advisor.py         # ğŸ’¼ Portfolio management
â”‚   â”‚   â”œâ”€â”€ analyst.py         # ğŸ“° News & research
â”‚   â”‚   â”œâ”€â”€ guardian.py        # ğŸ›¡ï¸ Compliance disclaimers
â”‚   â”‚   â””â”€â”€ scribe.py          # âœï¸ Response synthesis
â”‚   â”œâ”€â”€ llm/                   # Multi-provider LLM abstraction
â”‚   â”‚   â”œâ”€â”€ adapter.py         # BaseLLMAdapter + factory
â”‚   â”‚   â”œâ”€â”€ openai_provider.py
â”‚   â”‚   â”œâ”€â”€ anthropic_provider.py
â”‚   â”‚   â””â”€â”€ google_provider.py
â”‚   â”œâ”€â”€ orchestration/         # LangGraph workflow
â”‚   â”‚   â”œâ”€â”€ state.py           # FinnieState TypedDict
â”‚   â”‚   â”œâ”€â”€ graph.py           # StateGraph definition
â”‚   â”‚   â””â”€â”€ nodes.py           # Node functions (parse_intent, execute_*)
â”‚   â”œâ”€â”€ mcp/                   # Model Context Protocol tools
â”‚   â”‚   â”œâ”€â”€ server.py          # MCPToolRegistry singleton
â”‚   â”‚   â””â”€â”€ tools/
â”‚   â”‚       â”œâ”€â”€ finance_tools.py  # get_stock_price, get_historical_data, etc.
â”‚   â”‚       â””â”€â”€ chart_tools.py    # create_price_chart, create_comparison_chart
â”‚   â”œâ”€â”€ ui/                    # Streamlit frontend
â”‚   â”‚   â”œâ”€â”€ app.py             # Main app (tabs, routing, chat)
â”‚   â”‚   â”œâ”€â”€ auth.py            # Google/GitHub OAuth + guest login
â”‚   â”‚   â”œâ”€â”€ voice.py           # TTS (edge-tts) + STT control
â”‚   â”‚   â””â”€â”€ stt_component/     # Custom Streamlit component for mic input
â”‚   â”‚       â””â”€â”€ index.html
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ main.py            # FastAPI REST endpoint
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_agents.py
â”‚   â”œâ”€â”€ test_llm_adapters.py
â”‚   â”œâ”€â”€ test_orchestration.py
â”‚   â””â”€â”€ eval/                  # DeepEval evaluation tests
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ TEST_GUIDE.md
â”‚   â””â”€â”€ CODE_WALKTHROUGH.md    # â† You are here
â”œâ”€â”€ .env                       # Environment variables (API keys, feature flags)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ pyproject.toml
```

---

## Module Deep Dives

### 1. Configuration â€” `src/config.py`

Uses **pydantic-settings** to load and validate all environment variables from `.env`.

| Setting | Source | Purpose |
|---------|--------|---------|
| `GOOGLE_API_KEY` | `.env` | LLM provider API key |
| `DEFAULT_LLM_PROVIDER` | `.env` | `openai`, `anthropic`, or `google` |
| `DEFAULT_LLM_MODEL` | `.env` | e.g. `gemini-2.5-flash-lite` |
| `GRAPHRAG_ENABLED` | `.env` | Feature flag for Neo4j knowledge graph |
| `VOICE_ENABLED` | `.env` | Feature flag for TTS/STT |

Key functions:
- `get_settings()` â€” Cached singleton, loads `.env` once
- `get_llm_api_key(provider)` â€” Returns the API key for a given provider
- `SUPPORTED_MODELS` â€” Dict of all available models per provider

---

### 2. Agent System â€” `src/agents/`

All agents inherit from `BaseFinnieAgent` which provides:
- Abstract properties: `name`, `description`, `system_prompt`, `emoji`
- `process(state)` â€” Main async processing method
- `_call_llm(state, messages)` â€” Helper to call the configured LLM
- `_extract_tickers(text)` â€” Regex-based ticker extraction from text

#### Agent Roster

| Agent | File | Trigger Patterns | Data Source | Uses LLM? |
|-------|------|-------------------|-------------|-----------|
| ğŸ“Š **Quant** | `quant.py` | Ticker symbols, "price of AAPL" | yFinance (single stock deep-dive) | âŒ Fast data-only path |
| ğŸ“š **Professor** | `professor.py` | "What is", "Explain" | LLM knowledge | âœ… For explanations |
| ğŸŒ **Scout** | `scout.py` | "Trending", "Market today", "Predict" | yFinance (multi-ticker scan) + LLM | âœ… For market analysis |
| ğŸ”® **Oracle** | `oracle.py` | "If I invest", "Project" | Monte Carlo sim | âœ… For interpretation |
| ğŸ’¼ **Advisor** | `advisor.py` | Portfolio queries | User portfolio | âœ… For advice |
| ğŸ“° **Analyst** | `analyst.py` | "News about" | LLM knowledge | âœ… For research |
| ğŸ›¡ï¸ **Guardian** | `guardian.py` | Always runs | Compliance rules | No (rule-based) |
| âœï¸ **Scribe** | `scribe.py` | Always runs | Agent outputs | âœ… For synthesis |

> [!NOTE]
> **Quant vs Scout â€” why both?** Quant is the **fast path** for specific stock queries ("What's AAPL at?") â€” it fetches deep single-stock data (P/E, EPS, 52W range, volume, sector) and returns immediately **without an LLM call**. Scout handles **broad market scans** ("What's trending?", "Predict Monday") â€” it fetches 9 tickers, computes gainers/losers, and feeds the data to the LLM for intelligent analysis.

#### Data Flow for a Query

```mermaid
sequenceDiagram
    participant U as User
    participant App as app.py
    participant R as generate_response()
    participant A as Agent
    participant L as LLM Adapter
    participant T as MCP Tools

    U->>App: "What's AAPL trading at?"
    App->>R: Route by pattern matching
    R->>A: Quant.process(state)
    A->>T: call_tool("get_stock_price", ticker="AAPL")
    T-->>A: {price: 278.12, change: +3.0%}
    A->>L: _call_llm(state, [market_data + question])
    L-->>A: "Apple is trading at $278.12..."
    A-->>R: {content: "...", data: {...}}
    R-->>App: formatted response
    App-->>U: Display in chat + TTS
```

---

### 3. LLM Adapter Layer â€” `src/llm/`

A provider-agnostic abstraction. All adapters implement `BaseLLMAdapter.chat()`.

```python
# Usage (inside agents)
adapter = get_llm_adapter(provider="google", model="gemini-2.5-flash-lite", api_key="...")
response = await adapter.chat(messages=[...], system_prompt="...")
```

| Provider | Adapter | Models |
|----------|---------|--------|
| OpenAI | `OpenAIAdapter` | GPT-4o, GPT-4o Mini, GPT-3.5 Turbo |
| Anthropic | `AnthropicAdapter` | Claude Sonnet 4, Claude 3.5, Claude 3 Haiku |
| Google | `GoogleAdapter` | Gemini 2.5 Flash Lite, 2.5 Flash, 2.0 Flash, 1.5 Pro |

---

### 4. Orchestration â€” `src/orchestration/`

Built on **LangGraph** with a `StateGraph` workflow.

- **`state.py`** â€” Defines `FinnieState` (TypedDict) with fields: `user_input`, `intent`, `llm_provider`, `llm_model`, `llm_api_key`, `agent_responses`, `final_response`, etc.
- **`graph.py`** â€” Builds the graph: `START â†’ parse_intent â†’ route â†’ agent â†’ guardian â†’ scribe â†’ aggregate â†’ END`
- **`nodes.py`** â€” Node wrapper functions that instantiate agents and call `agent.process(state)`

> **Note:** The main UI (`app.py`) currently uses **direct agent routing** via `generate_response()` rather than the full LangGraph pipeline. The graph is available for the FastAPI endpoint and advanced workflows.

---

### 5. MCP Tool Server â€” `src/mcp/`

Implements the **Model Context Protocol** for tool discovery and execution.

Registered tools:

| Tool | Description |
|------|-------------|
| `get_stock_price` | Real-time price via yFinance |
| `get_historical_data` | Historical OHLCV data |
| `get_company_info` | Company fundamentals |
| `get_sector_performance` | Sector ETF performance |
| `create_price_chart` | Plotly candlestick chart |
| `create_comparison_chart` | Multi-ticker comparison |
| `create_sector_heatmap` | Sector performance heatmap |

---

### 6. Memory Layer â€” `src/memory.py`

SQLite-backed persistence with three tables:

| Table | Purpose |
|-------|---------|
| `users` | User profiles (OAuth + guest) |
| `conversations` | Chat sessions with titles |
| `messages` | Individual messages with roles |

Key functions: `upsert_user()`, `create_conversation()`, `save_message()`, `get_messages()`, `auto_title_conversation()`

---

### 7. UI Layer â€” `src/ui/`

#### `app.py` â€” Main Streamlit Application

The largest file (~1350 lines). Contains:

| Section | Lines | Purpose |
|---------|-------|---------|
| `init_session_state()` | 39â€“77 | Initialize provider, model, API key from `.env` |
| `_process_chat_input()` | 368â€“405 | Shared handler for text + voice input |
| `render_chat_tab()` | 408â€“460 | Chat UI with voice controls, message history, TTS |
| `render_portfolio_tab()` | ~460â€“650 | Portfolio tracking with holdings management |
| `render_market_tab()` | ~650â€“850 | Market data, charts, sector heatmaps |
| `render_projections_tab()` | ~850â€“870 | Monte Carlo investment projections |
| `render_settings_tab()` | ~870â€“935 | LLM provider config, connection status |
| `generate_response()` | 941â€“1125 | **Intent router** â€” pattern-matches user input to agents |

#### `auth.py` â€” Authentication

Supports three login methods:
- **Google OAuth** (via `google-auth-oauthlib`)
- **GitHub OAuth** (via `requests`)
- **Guest mode** (anonymous, no persistence)

#### `voice.py` â€” Voice Interface

| Function | Purpose |
|----------|---------|
| `render_voice_controls()` | Toggle, voice selector (6 voices), auto-speak |
| `speak_response(text, voice)` | TTS via `edge-tts` â†’ base64 `<audio controls autoplay>` |
| `render_stt_component()` | Custom Streamlit component for browser speech-to-text |
| `text_to_speech_sync()` | Async `edge-tts` wrapper using `asyncio.run()` |

**STT Architecture:** Uses `declare_component()` for proper bi-directional communication (not `components.html` which is one-way). The component in `stt_component/index.html` uses the Web Speech API and sends transcripts back via `setComponentValue`.

---

### 8. Observability â€” `src/observability.py`

**LangFuse** integration for production tracing:
- `FinnieObserver` singleton with `create_trace()`, `span()`, `end_trace()`
- Tracks latency, token usage, and errors per agent
- Falls back to local storage when LangFuse credentials aren't configured

---

### 9. FastAPI â€” `src/api/main.py`

REST API with endpoints:

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/health` | GET | Health check |
| `/chat` | POST | Single query (uses full LangGraph pipeline) |
| `/chat/stream` | POST | Streaming response via SSE |
| `/tools` | GET | List available MCP tools |
| `/tools/{name}` | POST | Execute a specific tool |

---

---

### 10. Evaluation Tests â€” `tests/eval/test_agent_quality.py`

Uses the **DeepEval** framework for LLM response quality evaluation. Tests run with:

```bash
pytest tests/eval/test_agent_quality.py -v
```

#### Metrics Tested

| Metric | What It Measures | Agents Tested |
|--------|-----------------|---------------|
| **Answer Relevancy** | Does the response address the query? | Quant, Professor, Advisor |
| **Hallucination** | Does the response fabricate data? | Quant, Professor |
| **Faithfulness** | Is the response grounded in retrieved context? | All agents |
| **Bias** | Does the response show unwanted bias? | All agents |

#### Test Data Structure

Each test case includes:
- `input` â€” The user's question (e.g., "What's AAPL trading at?")
- `expected_context` â€” Ground truth facts the response should align with
- `expected_output` â€” The expected formatted response for comparison

Tests cover scenarios for **Quant** (single stock, multi-stock comparison), **Professor** (P/E ratio, dollar cost averaging), and **Advisor** (portfolio diversification).

#### Additional Test Suites

| Suite | File | Tests |
|-------|------|-------|
| **MCP Tool Tests** | `test_agent_quality.py` | Tool discovery, schema validation, unknown tool handling |
| **Agent Unit Tests** | `test_agents.py` | Agent instantiation, ticker extraction, process() |
| **LLM Adapter Tests** | `test_llm_adapters.py` | Provider factory, API calls, error handling |
| **Orchestration Tests** | `test_orchestration.py` | Graph compilation, state management, intent routing |

---

## Key Data Flows

### Chat Input â†’ Response

```
User types/speaks â†’ _process_chat_input()
  â”œâ”€â”€ Save user message to SQLite
  â”œâ”€â”€ generate_response(user_input)
  â”‚   â”œâ”€â”€ Pattern-match intent (greeting? education? market? projection?)
  â”‚   â”œâ”€â”€ Instantiate the right Agent
  â”‚   â”œâ”€â”€ Build state dict {user_input, llm_provider, llm_model, llm_api_key}
  â”‚   â””â”€â”€ agent.process(state)
  â”‚       â”œâ”€â”€ Fetch data (yFinance, Monte Carlo, etc.)
  â”‚       â”œâ”€â”€ _call_llm(state, messages)  â† feeds data to LLM for analysis
  â”‚       â””â”€â”€ Return {content, data}
  â”œâ”€â”€ Save assistant message to SQLite
  â”œâ”€â”€ Flag TTS in session_state._speak_next (if voice on)
  â””â”€â”€ st.rerun()

Next render cycle:
  â”œâ”€â”€ Display all messages
  â”œâ”€â”€ If _speak_next â†’ speak_response() â†’ <audio controls autoplay>
  â””â”€â”€ Clear _speak_next
```

### API Key Flow

```
.env (GOOGLE_API_KEY=AIza...) 
  â†’ config.py get_settings() 
  â†’ app.py init_session_state() loads via get_llm_api_key()
  â†’ st.session_state.llm_api_key
  â†’ generate_response() passes to agent state dict
  â†’ agent._call_llm() â†’ get_llm_adapter(api_key=...)
  â†’ GoogleAdapter.chat()
```

---

## Environment Setup

```bash
# 1. Clone and install
git clone <repo> && cd finnie-ai
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# 2. Configure .env (minimum: one LLM API key)
cp .env.example .env
# Edit .env: set GOOGLE_API_KEY, DEFAULT_LLM_PROVIDER=google, DEFAULT_LLM_MODEL=gemini-2.5-flash-lite

# 3. Run
streamlit run src/ui/app.py
```

---

## Feature Status

| Feature | Status | Notes |
|---------|--------|-------|
| âœ… Market Data | Active | yFinance, no key needed |
| âœ… Monte Carlo Projections | Active | Built-in simulation engine |
| âœ… Portfolio Tracking | Active | SQLite persistence |
| âœ… LLM Chat | Active | Requires API key in `.env` |
| âœ… Voice Interface (TTS + STT) | Active | edge-tts + Web Speech API |
| âœ… MCP Tools | Active | 7 tools registered |
| âœ… FastAPI REST API | Active | `/chat`, `/tools` endpoints |
| âœ… LangFuse Observability | Optional | Needs LangFuse credentials |
| â¬œ GraphRAG | Not connected | Needs AuraDB (Neo4j) setup |
| âœ… Docker Deployment | Available | `Dockerfile` + `cloudbuild.yaml` |

---

*This document is maintained alongside the codebase. Update it when implementing new features or changing architecture.*
